Logging to F:\Pycharm_project\RL\logs\PPO\1118-215803
== CURRENT SYSTEM INFO ==
- OS: Windows-10-10.0.22000-SP0 10.0.22000
- Python: 3.10.12
- Stable-Baselines3: 2.1.0
- PyTorch: 2.1.0+cpu
- GPU Enabled: False
- Numpy: 1.25.0
- Cloudpickle: 2.2.1
- Gymnasium: 0.28.1
- OpenAI Gym: 0.26.2

== SAVED MODEL SYSTEM INFO ==
- OS: Windows-10-10.0.22000-SP0 10.0.22000
- Python: 3.10.12
- Stable-Baselines3: 2.1.0
- PyTorch: 2.1.0+cpu
- GPU Enabled: False
- Numpy: 1.25.0
- Cloudpickle: 2.2.1
- Gymnasium: 0.28.1
- OpenAI Gym: 0.26.2

-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.06e+03   |
|    std_reward           | 126         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007117684 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.69e+04    |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.00212    |
|    std                  | 0.891       |
|    value_loss           | 3.41e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.13e+03    |
|    std_reward           | 173          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023279772 |
|    clip_fraction        | 0.000513     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.81e+04     |
|    n_updates            | 10020        |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.892        |
|    value_loss           | 3.64e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.1e+03     |
|    std_reward           | 120          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0011547084 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86e+04     |
|    n_updates            | 10030        |
|    policy_gradient_loss | -0.000253    |
|    std                  | 0.891        |
|    value_loss           | 3.77e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -680        |
|    std_reward           | 630         |
|    success_rate         | 0.1         |
| train/                  |             |
|    approx_kl            | 0.009206768 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8e+04     |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.892       |
|    value_loss           | 3.51e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -956         |
|    std_reward           | 175          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0028133309 |
|    clip_fraction        | 0.000757     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.58e+04     |
|    n_updates            | 10050        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 0.89         |
|    value_loss           | 3.29e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -951         |
|    std_reward           | 125          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0022288102 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.66e+04     |
|    n_updates            | 10060        |
|    policy_gradient_loss | -0.000538    |
|    std                  | 0.889        |
|    value_loss           | 3.3e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -980         |
|    std_reward           | 170          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0012158139 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.05e+04     |
|    n_updates            | 10070        |
|    policy_gradient_loss | -0.000387    |
|    std                  | 0.889        |
|    value_loss           | 3.72e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -885         |
|    std_reward           | 112          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0014346819 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.57e+04     |
|    n_updates            | 10080        |
|    policy_gradient_loss | -0.000672    |
|    std                  | 0.89         |
|    value_loss           | 3.38e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -848         |
|    std_reward           | 26.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0054568592 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56e+04     |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.892        |
|    value_loss           | 3.32e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -870         |
|    std_reward           | 76.8         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034031263 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7e+04      |
|    n_updates            | 10100        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 0.893        |
|    value_loss           | 3.36e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -863        |
|    std_reward           | 51.4        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002091391 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.69e+04    |
|    n_updates            | 10110       |
|    policy_gradient_loss | -0.000668   |
|    std                  | 0.894       |
|    value_loss           | 3.42e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1e+03       |
|    std_reward           | 70.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0042614928 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.68e+04     |
|    n_updates            | 10120        |
|    policy_gradient_loss | -0.0014      |
|    std                  | 0.893        |
|    value_loss           | 3.39e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -893         |
|    std_reward           | 66.3         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034795585 |
|    clip_fraction        | 0.00203      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.71e+04     |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.893        |
|    value_loss           | 3.62e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -956        |
|    std_reward           | 174         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004323579 |
|    clip_fraction        | 0.00918     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.77e+04    |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.000959   |
|    std                  | 0.893       |
|    value_loss           | 3.4e+04     |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -839         |
|    std_reward           | 47.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023668262 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.67e+04     |
|    n_updates            | 10150        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 0.894        |
|    value_loss           | 3.48e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -924         |
|    std_reward           | 135          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0019423983 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.66e+04     |
|    n_updates            | 10160        |
|    policy_gradient_loss | -0.000644    |
|    std                  | 0.894        |
|    value_loss           | 3.32e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.01e+03    |
|    std_reward           | 107          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0037468649 |
|    clip_fraction        | 0.00393      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7e+04      |
|    n_updates            | 10170        |
|    policy_gradient_loss | -0.00161     |
|    std                  | 0.894        |
|    value_loss           | 3.4e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -966         |
|    std_reward           | 87.7         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0006851413 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.76e+04     |
|    n_updates            | 10180        |
|    policy_gradient_loss | -0.000145    |
|    std                  | 0.894        |
|    value_loss           | 3.52e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.08e+03    |
|    std_reward           | 289          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0036338745 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.86e+04     |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.893        |
|    value_loss           | 3.63e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.03e+03    |
|    std_reward           | 279          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034972318 |
|    clip_fraction        | 0.00295      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.74e+04     |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 0.892        |
|    value_loss           | 3.38e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -975         |
|    std_reward           | 262          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0048457934 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.79e+04     |
|    n_updates            | 10210        |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.891        |
|    value_loss           | 3.62e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -907        |
|    std_reward           | 120         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002181241 |
|    clip_fraction        | 0.000293    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.79e+04    |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.000767   |
|    std                  | 0.891       |
|    value_loss           | 3.44e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -904         |
|    std_reward           | 115          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0057641035 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.63e+04     |
|    n_updates            | 10230        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.89         |
|    value_loss           | 3.38e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -968        |
|    std_reward           | 159         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002674729 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.65e+04    |
|    n_updates            | 10240       |
|    policy_gradient_loss | -0.000995   |
|    std                  | 0.89        |
|    value_loss           | 3.4e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1e+03      |
|    std_reward           | 190         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002025601 |
|    clip_fraction        | 0.000708    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.75e+04    |
|    n_updates            | 10250       |
|    policy_gradient_loss | -0.000487   |
|    std                  | 0.89        |
|    value_loss           | 3.41e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -883         |
|    std_reward           | 135          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0035469877 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10260        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 0.891        |
|    value_loss           | 3.31e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -890        |
|    std_reward           | 72.3        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006283925 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.73e+04    |
|    n_updates            | 10270       |
|    policy_gradient_loss | -0.00228    |
|    std                  | 0.892       |
|    value_loss           | 3.3e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -887        |
|    std_reward           | 67.4        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002369144 |
|    clip_fraction        | 0.00061     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.52e+04    |
|    n_updates            | 10280       |
|    policy_gradient_loss | -0.000928   |
|    std                  | 0.892       |
|    value_loss           | 3.28e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -984        |
|    std_reward           | 88.4        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003935245 |
|    clip_fraction        | 0.00413     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68e+04    |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.892       |
|    value_loss           | 3.23e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -909         |
|    std_reward           | 116          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0025422783 |
|    clip_fraction        | 0.000513     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.65e+04     |
|    n_updates            | 10300        |
|    policy_gradient_loss | -0.000948    |
|    std                  | 0.89         |
|    value_loss           | 3.19e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -908         |
|    std_reward           | 124          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0029975392 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.6e+04      |
|    n_updates            | 10310        |
|    policy_gradient_loss | -0.000958    |
|    std                  | 0.889        |
|    value_loss           | 3.22e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.02e+03   |
|    std_reward           | 198         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007540162 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57e+04    |
|    n_updates            | 10320       |
|    policy_gradient_loss | -0.00286    |
|    std                  | 0.89        |
|    value_loss           | 3.2e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.2e+03    |
|    std_reward           | 278         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006592055 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04e+04    |
|    n_updates            | 10330       |
|    policy_gradient_loss | -0.00186    |
|    std                  | 0.89        |
|    value_loss           | 3.99e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.19e+03    |
|    std_reward           | 243          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0009570342 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 2.04e+04     |
|    n_updates            | 10340        |
|    policy_gradient_loss | -0.000549    |
|    std                  | 0.89         |
|    value_loss           | 3.9e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.1e+03     |
|    std_reward           | 191          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031335424 |
|    clip_fraction        | 0.00212      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.76e+04     |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.89         |
|    value_loss           | 3.48e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.17e+03    |
|    std_reward           | 269          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0021621638 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7e+04      |
|    n_updates            | 10360        |
|    policy_gradient_loss | -0.000849    |
|    std                  | 0.89         |
|    value_loss           | 3.57e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -951         |
|    std_reward           | 101          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038757618 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69e+04     |
|    n_updates            | 10370        |
|    policy_gradient_loss | -0.0013      |
|    std                  | 0.891        |
|    value_loss           | 3.4e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -990         |
|    std_reward           | 119          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0061146948 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.87e+04     |
|    n_updates            | 10380        |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.892        |
|    value_loss           | 3.66e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.01e+03   |
|    std_reward           | 143         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003420427 |
|    clip_fraction        | 0.00232     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.6e+04     |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00116    |
|    std                  | 0.89        |
|    value_loss           | 3.18e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -999         |
|    std_reward           | 158          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0014329694 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.000544    |
|    std                  | 0.888        |
|    value_loss           | 3.34e+04     |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_reward          | -1.02e+03     |
|    std_reward           | 118           |
|    success_rate         | 0             |
| train/                  |               |
|    approx_kl            | 0.00043885555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.77         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.69e+04      |
|    n_updates            | 10410         |
|    policy_gradient_loss | -0.000227     |
|    std                  | 0.888         |
|    value_loss           | 3.45e+04      |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -987         |
|    std_reward           | 165          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0028059727 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.888        |
|    value_loss           | 3.35e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.01e+03    |
|    std_reward           | 163          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018766519 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.65e+04     |
|    n_updates            | 10430        |
|    policy_gradient_loss | -0.00061     |
|    std                  | 0.888        |
|    value_loss           | 3.28e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -973         |
|    std_reward           | 110          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0019212309 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.64e+04     |
|    n_updates            | 10440        |
|    policy_gradient_loss | -0.000436    |
|    std                  | 0.888        |
|    value_loss           | 3.22e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -915         |
|    std_reward           | 96.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0028810005 |
|    clip_fraction        | 0.00173      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 10450        |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.888        |
|    value_loss           | 3.18e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.16e+03  |
|    std_reward           | 209        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00652025 |
|    clip_fraction        | 0.0155     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.77      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65e+04   |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.00283   |
|    std                  | 0.889      |
|    value_loss           | 3.3e+04    |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.01e+03    |
|    std_reward           | 117          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0045720832 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.53e+04     |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.889        |
|    value_loss           | 3.1e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 178          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0055421004 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10480        |
|    policy_gradient_loss | -0.00207     |
|    std                  | 0.891        |
|    value_loss           | 3.27e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -905         |
|    std_reward           | 61.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0028793989 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.891        |
|    value_loss           | 3.26e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.01e+03    |
|    std_reward           | 124          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033856104 |
|    clip_fraction        | 0.00188      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.79        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7e+04      |
|    n_updates            | 10500        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.89         |
|    value_loss           | 3.25e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.17e+03    |
|    std_reward           | 204          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0016593885 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.78        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.74e+04     |
|    n_updates            | 10510        |
|    policy_gradient_loss | -0.000632    |
|    std                  | 0.888        |
|    value_loss           | 3.32e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.05e+03   |
|    std_reward           | 173         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004543591 |
|    clip_fraction        | 0.005       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72e+04    |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.888       |
|    value_loss           | 3.4e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.13e+03   |
|    std_reward           | 210         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002695438 |
|    clip_fraction        | 0.00149     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68e+04    |
|    n_updates            | 10530       |
|    policy_gradient_loss | -0.000691   |
|    std                  | 0.887       |
|    value_loss           | 3.28e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.02e+03   |
|    std_reward           | 163         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002738709 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.68e+04    |
|    n_updates            | 10540       |
|    policy_gradient_loss | -0.000847   |
|    std                  | 0.887       |
|    value_loss           | 3.31e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 169          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0022781594 |
|    clip_fraction        | 0.000708     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.89e+04     |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.000686    |
|    std                  | 0.887        |
|    value_loss           | 3.58e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.12e+03    |
|    std_reward           | 213          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0029319339 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.57e+04     |
|    n_updates            | 10560        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 0.887        |
|    value_loss           | 3.2e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -970         |
|    std_reward           | 174          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0032724035 |
|    clip_fraction        | 0.00203      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7e+04      |
|    n_updates            | 10570        |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.887        |
|    value_loss           | 3.33e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -947        |
|    std_reward           | 158         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004622852 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.887       |
|    value_loss           | 3.05e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -996        |
|    std_reward           | 145         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003137691 |
|    clip_fraction        | 0.00164     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5e+04     |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.000779   |
|    std                  | 0.886       |
|    value_loss           | 2.96e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -997        |
|    std_reward           | 154         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004237169 |
|    clip_fraction        | 0.00537     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48e+04    |
|    n_updates            | 10600       |
|    policy_gradient_loss | -0.00131    |
|    std                  | 0.885       |
|    value_loss           | 3.07e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -786         |
|    std_reward           | 666          |
|    success_rate         | 0.1          |
| train/                  |              |
|    approx_kl            | 0.0045689847 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.885        |
|    value_loss           | 3.02e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -988         |
|    std_reward           | 81.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0035718651 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.45e+04     |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.883        |
|    value_loss           | 2.99e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.14e+03    |
|    std_reward           | 111          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018933521 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.6e+04      |
|    n_updates            | 10630        |
|    policy_gradient_loss | -0.000925    |
|    std                  | 0.883        |
|    value_loss           | 3.03e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -968         |
|    std_reward           | 101          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0036063823 |
|    clip_fraction        | 0.0032       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10640        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.884        |
|    value_loss           | 3.27e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.17e+03   |
|    std_reward           | 116         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007284818 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49e+04    |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.884       |
|    value_loss           | 2.93e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.19e+03    |
|    std_reward           | 111          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0005649375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.58e+04     |
|    n_updates            | 10660        |
|    policy_gradient_loss | -0.000329    |
|    std                  | 0.884        |
|    value_loss           | 3.19e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 121          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0026990701 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72e+04     |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.000955    |
|    std                  | 0.884        |
|    value_loss           | 3.33e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -979         |
|    std_reward           | 105          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0050587077 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.57e+04     |
|    n_updates            | 10680        |
|    policy_gradient_loss | -0.00189     |
|    std                  | 0.884        |
|    value_loss           | 2.94e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -960         |
|    std_reward           | 95.5         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031017717 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+04     |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.883        |
|    value_loss           | 2.95e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.04e+03   |
|    std_reward           | 124         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006525471 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54e+04    |
|    n_updates            | 10700       |
|    policy_gradient_loss | -0.00177    |
|    std                  | 0.883       |
|    value_loss           | 3e+04       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.13e+03    |
|    std_reward           | 119          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0045092166 |
|    clip_fraction        | 0.00701      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.71e+04     |
|    n_updates            | 10710        |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.882        |
|    value_loss           | 3.23e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 155          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0032762946 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.61e+04     |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.000929    |
|    std                  | 0.883        |
|    value_loss           | 3.26e+04     |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_reward          | -953          |
|    std_reward           | 131           |
|    success_rate         | 0             |
| train/                  |               |
|    approx_kl            | 0.00078410184 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.73         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.48e+04      |
|    n_updates            | 10730         |
|    policy_gradient_loss | -0.000313     |
|    std                  | 0.882         |
|    value_loss           | 3.04e+04      |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -992         |
|    std_reward           | 126          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0025841217 |
|    clip_fraction        | 0.000708     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+04     |
|    n_updates            | 10740        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.882        |
|    value_loss           | 3.01e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.01e+03   |
|    std_reward           | 112         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008155391 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51e+04    |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.00284    |
|    std                  | 0.882       |
|    value_loss           | 3.05e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -983         |
|    std_reward           | 146          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0013537754 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+04     |
|    n_updates            | 10760        |
|    policy_gradient_loss | -0.000651    |
|    std                  | 0.883        |
|    value_loss           | 3.09e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.12e+03   |
|    std_reward           | 145         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004276418 |
|    clip_fraction        | 0.00505     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63e+04    |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.00141    |
|    std                  | 0.883       |
|    value_loss           | 3.13e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.08e+03    |
|    std_reward           | 166          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0020411015 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56e+04     |
|    n_updates            | 10780        |
|    policy_gradient_loss | -0.000582    |
|    std                  | 0.883        |
|    value_loss           | 3.12e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.21e+03   |
|    std_reward           | 145         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002778962 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66e+04    |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.00136    |
|    std                  | 0.884       |
|    value_loss           | 3.34e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.04e+03    |
|    std_reward           | 176          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0035531807 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 10800        |
|    policy_gradient_loss | -0.000911    |
|    std                  | 0.885        |
|    value_loss           | 3.19e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -955         |
|    std_reward           | 125          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033716296 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.75        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 10810        |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.886        |
|    value_loss           | 3.22e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -910        |
|    std_reward           | 98.5        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003371431 |
|    clip_fraction        | 0.00188     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38e+04    |
|    n_updates            | 10820       |
|    policy_gradient_loss | -0.000796   |
|    std                  | 0.885       |
|    value_loss           | 2.9e+04     |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -871         |
|    std_reward           | 52.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0020097753 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 10830        |
|    policy_gradient_loss | -0.000908    |
|    std                  | 0.884        |
|    value_loss           | 3.01e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -718         |
|    std_reward           | 641          |
|    success_rate         | 0.1          |
| train/                  |              |
|    approx_kl            | 0.0039220066 |
|    clip_fraction        | 0.00354      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 10840        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.882        |
|    value_loss           | 3.04e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -910         |
|    std_reward           | 48.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031644222 |
|    clip_fraction        | 0.00256      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.6e+04      |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.000652    |
|    std                  | 0.882        |
|    value_loss           | 3.13e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -900        |
|    std_reward           | 74.7        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004127462 |
|    clip_fraction        | 0.00364     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57e+04    |
|    n_updates            | 10860       |
|    policy_gradient_loss | -0.00166    |
|    std                  | 0.881       |
|    value_loss           | 3.17e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.02e+03    |
|    std_reward           | 190          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0024771935 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.54e+04     |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.000795    |
|    std                  | 0.883        |
|    value_loss           | 3.02e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -971        |
|    std_reward           | 195         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005536418 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49e+04    |
|    n_updates            | 10880       |
|    policy_gradient_loss | -0.00141    |
|    std                  | 0.883       |
|    value_loss           | 2.98e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.07e+03    |
|    std_reward           | 144          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0066916607 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00296     |
|    std                  | 0.882        |
|    value_loss           | 2.98e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.16e+03  |
|    std_reward           | 185        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00897431 |
|    clip_fraction        | 0.042      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.72      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53e+04   |
|    n_updates            | 10900      |
|    policy_gradient_loss | -0.00308   |
|    std                  | 0.881      |
|    value_loss           | 2.95e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.18e+03    |
|    std_reward           | 79           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0064739967 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.54e+04     |
|    n_updates            | 10910        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.881        |
|    value_loss           | 3.01e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.03e+03    |
|    std_reward           | 155          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0016831423 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 10920        |
|    policy_gradient_loss | -0.000873    |
|    std                  | 0.881        |
|    value_loss           | 2.96e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -967        |
|    std_reward           | 146         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002523725 |
|    clip_fraction        | 0.001       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48e+04    |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.00106    |
|    std                  | 0.882       |
|    value_loss           | 2.86e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -974         |
|    std_reward           | 130          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0059135705 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+04     |
|    n_updates            | 10940        |
|    policy_gradient_loss | -0.00139     |
|    std                  | 0.881        |
|    value_loss           | 2.88e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.07e+03   |
|    std_reward           | 165         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007886354 |
|    clip_fraction        | 0.038       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38e+04    |
|    n_updates            | 10950       |
|    policy_gradient_loss | -0.00289    |
|    std                  | 0.882       |
|    value_loss           | 2.9e+04     |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -966         |
|    std_reward           | 83.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038210577 |
|    clip_fraction        | 0.00525      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+04     |
|    n_updates            | 10960        |
|    policy_gradient_loss | -0.000954    |
|    std                  | 0.883        |
|    value_loss           | 2.9e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -923         |
|    std_reward           | 120          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0019644215 |
|    clip_fraction        | 0.000659     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 10970        |
|    policy_gradient_loss | -0.0008      |
|    std                  | 0.883        |
|    value_loss           | 2.93e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -958        |
|    std_reward           | 123         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007787735 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53e+04    |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.883       |
|    value_loss           | 2.89e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -875         |
|    std_reward           | 53.3         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0029776106 |
|    clip_fraction        | 0.00164      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.63e+04     |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00133     |
|    std                  | 0.883        |
|    value_loss           | 3.33e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -899        |
|    std_reward           | 79.6        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004826215 |
|    clip_fraction        | 0.00789     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 11000       |
|    policy_gradient_loss | -0.00124    |
|    std                  | 0.884       |
|    value_loss           | 3.03e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1e+03       |
|    std_reward           | 108          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031582778 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 11010        |
|    policy_gradient_loss | -0.000864    |
|    std                  | 0.884        |
|    value_loss           | 2.99e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -963         |
|    std_reward           | 65           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0016667629 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 11020        |
|    policy_gradient_loss | -0.000708    |
|    std                  | 0.883        |
|    value_loss           | 3.05e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -906        |
|    std_reward           | 59          |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006839921 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41e+04    |
|    n_updates            | 11030       |
|    policy_gradient_loss | -0.00277    |
|    std                  | 0.882       |
|    value_loss           | 2.87e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -873        |
|    std_reward           | 50.8        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005814315 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47e+04    |
|    n_updates            | 11040       |
|    policy_gradient_loss | -0.00218    |
|    std                  | 0.881       |
|    value_loss           | 3.03e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -943        |
|    std_reward           | 68          |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004717541 |
|    clip_fraction        | 0.00735     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.78e+04    |
|    n_updates            | 11050       |
|    policy_gradient_loss | -0.00169    |
|    std                  | 0.88        |
|    value_loss           | 3.48e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.21e+03   |
|    std_reward           | 249         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006534272 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47e+04    |
|    n_updates            | 11060       |
|    policy_gradient_loss | -0.00153    |
|    std                  | 0.88        |
|    value_loss           | 3.04e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -976        |
|    std_reward           | 91.1        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005750848 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72e+04    |
|    n_updates            | 11070       |
|    policy_gradient_loss | -0.00204    |
|    std                  | 0.879       |
|    value_loss           | 3.36e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -909        |
|    std_reward           | 73.7        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006805827 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72e+04    |
|    n_updates            | 11080       |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.879       |
|    value_loss           | 3.31e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -961         |
|    std_reward           | 147          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018268714 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+04     |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.000755    |
|    std                  | 0.878        |
|    value_loss           | 3.01e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -908         |
|    std_reward           | 98.7         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0046587596 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 11100        |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.879        |
|    value_loss           | 3.1e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -923         |
|    std_reward           | 95.8         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033630026 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 11110        |
|    policy_gradient_loss | -0.0012      |
|    std                  | 0.879        |
|    value_loss           | 3e+04        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -917        |
|    std_reward           | 134         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006108069 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49e+04    |
|    n_updates            | 11120       |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.878       |
|    value_loss           | 3.07e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -930        |
|    std_reward           | 91.9        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005171477 |
|    clip_fraction        | 0.00891     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.65e+04    |
|    n_updates            | 11130       |
|    policy_gradient_loss | -0.00141    |
|    std                  | 0.879       |
|    value_loss           | 3.21e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.01e+03   |
|    std_reward           | 131         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003939541 |
|    clip_fraction        | 0.00427     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59e+04    |
|    n_updates            | 11140       |
|    policy_gradient_loss | -0.00105    |
|    std                  | 0.879       |
|    value_loss           | 3.19e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.01e+03    |
|    std_reward           | 115          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0062755058 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+04     |
|    n_updates            | 11150        |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.879        |
|    value_loss           | 3.16e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -919         |
|    std_reward           | 92.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0011725349 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.58e+04     |
|    n_updates            | 11160        |
|    policy_gradient_loss | -0.000217    |
|    std                  | 0.88         |
|    value_loss           | 3.38e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.11e+03   |
|    std_reward           | 113         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007327616 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.66e+04    |
|    n_updates            | 11170       |
|    policy_gradient_loss | -0.00235    |
|    std                  | 0.882       |
|    value_loss           | 3.33e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.15e+03    |
|    std_reward           | 180          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0011436088 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.78e+04     |
|    n_updates            | 11180        |
|    policy_gradient_loss | -0.000407    |
|    std                  | 0.881        |
|    value_loss           | 3.49e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.03e+03    |
|    std_reward           | 250          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0064996993 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.77e+04     |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.883        |
|    value_loss           | 3.67e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -984        |
|    std_reward           | 125         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006310411 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71e+04    |
|    n_updates            | 11200       |
|    policy_gradient_loss | -0.0019     |
|    std                  | 0.885       |
|    value_loss           | 3.39e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -908        |
|    std_reward           | 136         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002350601 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92e+04    |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.000789   |
|    std                  | 0.885       |
|    value_loss           | 3.75e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -858        |
|    std_reward           | 81.2        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006730103 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58e+04    |
|    n_updates            | 11220       |
|    policy_gradient_loss | -0.00211    |
|    std                  | 0.884       |
|    value_loss           | 3.22e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -900         |
|    std_reward           | 105          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018598647 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.58e+04     |
|    n_updates            | 11230        |
|    policy_gradient_loss | -0.000608    |
|    std                  | 0.883        |
|    value_loss           | 3.29e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -957        |
|    std_reward           | 162         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006548944 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47e+04    |
|    n_updates            | 11240       |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.882       |
|    value_loss           | 3.21e+04    |
-----------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_reward          | -1.03e+03 |
|    std_reward           | 121       |
|    success_rate         | 0         |
| train/                  |           |
|    approx_kl            | 0.0021711 |
|    clip_fraction        | 0.000342  |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.71     |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0003    |
|    loss                 | 1.57e+04  |
|    n_updates            | 11250     |
|    policy_gradient_loss | -0.00105  |
|    std                  | 0.881     |
|    value_loss           | 3.16e+04  |
---------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -938         |
|    std_reward           | 107          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0061417287 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+04     |
|    n_updates            | 11260        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.879        |
|    value_loss           | 3.07e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.07e+03    |
|    std_reward           | 181          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0071220323 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55e+04     |
|    n_updates            | 11270        |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.877        |
|    value_loss           | 3.09e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -927         |
|    std_reward           | 157          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031140242 |
|    clip_fraction        | 0.00139      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.64e+04     |
|    n_updates            | 11280        |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.876        |
|    value_loss           | 3.26e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.05e+03   |
|    std_reward           | 180         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006222794 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.55e+04    |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.875       |
|    value_loss           | 3.1e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.06e+03   |
|    std_reward           | 192         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006042471 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.49e+04    |
|    n_updates            | 11300       |
|    policy_gradient_loss | -0.00207    |
|    std                  | 0.875       |
|    value_loss           | 2.92e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.29e+03    |
|    std_reward           | 281          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0021818685 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 11310        |
|    policy_gradient_loss | -0.0006      |
|    std                  | 0.874        |
|    value_loss           | 2.95e+04     |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_reward          | -1.23e+03     |
|    std_reward           | 281           |
|    success_rate         | 0             |
| train/                  |               |
|    approx_kl            | 0.00094137486 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.67         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0003        |
|    loss                 | 1.48e+04      |
|    n_updates            | 11320         |
|    policy_gradient_loss | -0.000255     |
|    std                  | 0.875         |
|    value_loss           | 3.08e+04      |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -998        |
|    std_reward           | 198         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006531108 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58e+04    |
|    n_updates            | 11330       |
|    policy_gradient_loss | -0.00249    |
|    std                  | 0.875       |
|    value_loss           | 3.06e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -921        |
|    std_reward           | 85.9        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008122899 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.67       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45e+04    |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.00311    |
|    std                  | 0.874       |
|    value_loss           | 2.91e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -945         |
|    std_reward           | 104          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034664522 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 11350        |
|    policy_gradient_loss | -0.000942    |
|    std                  | 0.875        |
|    value_loss           | 2.95e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -986        |
|    std_reward           | 219         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.001434505 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53e+04    |
|    n_updates            | 11360       |
|    policy_gradient_loss | -0.000598   |
|    std                  | 0.877       |
|    value_loss           | 3.03e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.31e+03    |
|    std_reward           | 132          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0070486926 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 11370        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.877        |
|    value_loss           | 3e+04        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -949         |
|    std_reward           | 90.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0063480707 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.58e+04     |
|    n_updates            | 11380        |
|    policy_gradient_loss | -0.0022      |
|    std                  | 0.875        |
|    value_loss           | 3.08e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.14e+03    |
|    std_reward           | 111          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0050482573 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.45e+04     |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.873        |
|    value_loss           | 3.01e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.27e+03   |
|    std_reward           | 161         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004452221 |
|    clip_fraction        | 0.00566     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.66       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37e+04    |
|    n_updates            | 11400       |
|    policy_gradient_loss | -0.000995   |
|    std                  | 0.872       |
|    value_loss           | 2.84e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.35e+03    |
|    std_reward           | 196          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0016277423 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.61e+04     |
|    n_updates            | 11410        |
|    policy_gradient_loss | -0.000553    |
|    std                  | 0.873        |
|    value_loss           | 3.03e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.33e+03    |
|    std_reward           | 217          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0017698803 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 11420        |
|    policy_gradient_loss | -0.000566    |
|    std                  | 0.873        |
|    value_loss           | 3.09e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.28e+03  |
|    std_reward           | 209        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00318293 |
|    clip_fraction        | 0.00178    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.66      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51e+04   |
|    n_updates            | 11430      |
|    policy_gradient_loss | -0.0014    |
|    std                  | 0.873      |
|    value_loss           | 3e+04      |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.17e+03    |
|    std_reward           | 195          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0024452233 |
|    clip_fraction        | 0.000903     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+04     |
|    n_updates            | 11440        |
|    policy_gradient_loss | -0.000731    |
|    std                  | 0.872        |
|    value_loss           | 2.9e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.05e+03   |
|    std_reward           | 208         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006585118 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.56e+04    |
|    n_updates            | 11450       |
|    policy_gradient_loss | -0.00193    |
|    std                  | 0.871       |
|    value_loss           | 3.04e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.26e+03   |
|    std_reward           | 257         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005456986 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.64e+04    |
|    n_updates            | 11460       |
|    policy_gradient_loss | -0.00144    |
|    std                  | 0.871       |
|    value_loss           | 3.12e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.06e+03    |
|    std_reward           | 193          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0064061163 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.59e+04     |
|    n_updates            | 11470        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.871        |
|    value_loss           | 3.14e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.18e+03    |
|    std_reward           | 197          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023680609 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+04     |
|    n_updates            | 11480        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 0.871        |
|    value_loss           | 2.94e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.15e+03    |
|    std_reward           | 194          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0032195833 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.48e+04     |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.000662    |
|    std                  | 0.87         |
|    value_loss           | 2.94e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.29e+03    |
|    std_reward           | 68.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0008693555 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 11500        |
|    policy_gradient_loss | -0.000402    |
|    std                  | 0.87         |
|    value_loss           | 2.97e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.24e+03    |
|    std_reward           | 129          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0065572886 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+04     |
|    n_updates            | 11510        |
|    policy_gradient_loss | -0.00167     |
|    std                  | 0.869        |
|    value_loss           | 2.99e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.12e+03    |
|    std_reward           | 145          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038768547 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.68e+04     |
|    n_updates            | 11520        |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.869        |
|    value_loss           | 3.3e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -968         |
|    std_reward           | 73.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0026951295 |
|    clip_fraction        | 0.000757     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+04     |
|    n_updates            | 11530        |
|    policy_gradient_loss | -0.000939    |
|    std                  | 0.869        |
|    value_loss           | 3.02e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -857         |
|    std_reward           | 28.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0065882006 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 11540        |
|    policy_gradient_loss | -0.0027      |
|    std                  | 0.869        |
|    value_loss           | 2.8e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -984        |
|    std_reward           | 107         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005844523 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37e+04    |
|    n_updates            | 11550       |
|    policy_gradient_loss | -0.00264    |
|    std                  | 0.868       |
|    value_loss           | 2.75e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.1e+03    |
|    std_reward           | 155         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003835401 |
|    clip_fraction        | 0.00327     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38e+04    |
|    n_updates            | 11560       |
|    policy_gradient_loss | -0.000983   |
|    std                  | 0.867       |
|    value_loss           | 2.79e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.02e+03   |
|    std_reward           | 174         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003585388 |
|    clip_fraction        | 0.00322     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.52e+04    |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.00116    |
|    std                  | 0.868       |
|    value_loss           | 2.96e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -940         |
|    std_reward           | 102          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0055367993 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.62        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.53e+04     |
|    n_updates            | 11580        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.867        |
|    value_loss           | 3.05e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -979        |
|    std_reward           | 180         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.000629316 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36e+04    |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.000189   |
|    std                  | 0.865       |
|    value_loss           | 2.86e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.11e+03    |
|    std_reward           | 205          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0070915865 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.54e+04     |
|    n_updates            | 11600        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 0.864        |
|    value_loss           | 3.07e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -998         |
|    std_reward           | 141          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018034956 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+04      |
|    n_updates            | 11610        |
|    policy_gradient_loss | -0.000815    |
|    std                  | 0.863        |
|    value_loss           | 2.98e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -984         |
|    std_reward           | 191          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033780283 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.59        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.53e+04     |
|    n_updates            | 11620        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.862        |
|    value_loss           | 3.06e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -895         |
|    std_reward           | 725          |
|    success_rate         | 0.1          |
| train/                  |              |
|    approx_kl            | 0.0017986668 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 11630        |
|    policy_gradient_loss | -0.000373    |
|    std                  | 0.861        |
|    value_loss           | 3.1e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -868        |
|    std_reward           | 48.1        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003512961 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63e+04    |
|    n_updates            | 11640       |
|    policy_gradient_loss | -0.000841   |
|    std                  | 0.862       |
|    value_loss           | 3.13e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -896         |
|    std_reward           | 52.2         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0056653093 |
|    clip_fraction        | 0.00999      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.2e+04      |
|    n_updates            | 11650        |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.861        |
|    value_loss           | 4.39e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -926         |
|    std_reward           | 74.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0036482147 |
|    clip_fraction        | 0.0032       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.61e+04     |
|    n_updates            | 11660        |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.86         |
|    value_loss           | 3.19e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -931        |
|    std_reward           | 45.2        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004106163 |
|    clip_fraction        | 0.005       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 11670       |
|    policy_gradient_loss | -0.000913   |
|    std                  | 0.859       |
|    value_loss           | 2.96e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -950        |
|    std_reward           | 48          |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004780759 |
|    clip_fraction        | 0.00859     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57e+04    |
|    n_updates            | 11680       |
|    policy_gradient_loss | -0.001      |
|    std                  | 0.859       |
|    value_loss           | 2.97e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -884         |
|    std_reward           | 70           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0067287046 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55e+04     |
|    n_updates            | 11690        |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.86         |
|    value_loss           | 2.84e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -948        |
|    std_reward           | 102         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004710978 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 11700       |
|    policy_gradient_loss | -0.0012     |
|    std                  | 0.86        |
|    value_loss           | 2.85e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -910        |
|    std_reward           | 67.4        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006234924 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.55e+04    |
|    n_updates            | 11710       |
|    policy_gradient_loss | -0.0016     |
|    std                  | 0.86        |
|    value_loss           | 3.04e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -888         |
|    std_reward           | 87.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0047155498 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.59e+04     |
|    n_updates            | 11720        |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.86         |
|    value_loss           | 3.33e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -867        |
|    std_reward           | 44.5        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005343946 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44e+04    |
|    n_updates            | 11730       |
|    policy_gradient_loss | -0.00197    |
|    std                  | 0.859       |
|    value_loss           | 2.91e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -867         |
|    std_reward           | 72.3         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0050406596 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47e+04     |
|    n_updates            | 11740        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.858        |
|    value_loss           | 2.85e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -861       |
|    std_reward           | 46.4       |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00524348 |
|    clip_fraction        | 0.0122     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.56      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49e+04   |
|    n_updates            | 11750      |
|    policy_gradient_loss | -0.00206   |
|    std                  | 0.857      |
|    value_loss           | 2.84e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -828         |
|    std_reward           | 31.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0021269037 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.56        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 11760        |
|    policy_gradient_loss | -0.000714    |
|    std                  | 0.858        |
|    value_loss           | 2.78e+04     |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_reward          | -843          |
|    std_reward           | 43.4          |
|    success_rate         | 0             |
| train/                  |               |
|    approx_kl            | 0.00039126078 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.56         |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.41e+04      |
|    n_updates            | 11770         |
|    policy_gradient_loss | -6.77e-05     |
|    std                  | 0.857         |
|    value_loss           | 2.81e+04      |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -888         |
|    std_reward           | 60.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0077746315 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.55        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.43e+04     |
|    n_updates            | 11780        |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.856        |
|    value_loss           | 2.95e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -928         |
|    std_reward           | 67.7         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0052245655 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35e+04     |
|    n_updates            | 11790        |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.855        |
|    value_loss           | 2.7e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -891         |
|    std_reward           | 49.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0072559156 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.34e+04     |
|    n_updates            | 11800        |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.855        |
|    value_loss           | 2.8e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -885         |
|    std_reward           | 33.7         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0046327678 |
|    clip_fraction        | 0.00725      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.33e+04     |
|    n_updates            | 11810        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 0.855        |
|    value_loss           | 2.71e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -936         |
|    std_reward           | 60.2         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0022383519 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+04      |
|    n_updates            | 11820        |
|    policy_gradient_loss | -0.000563    |
|    std                  | 0.854        |
|    value_loss           | 2.68e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -903        |
|    std_reward           | 71.9        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.007890232 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.61e+04    |
|    n_updates            | 11830       |
|    policy_gradient_loss | -0.00251    |
|    std                  | 0.851       |
|    value_loss           | 3.07e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -965         |
|    std_reward           | 91           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0047501675 |
|    clip_fraction        | 0.00701      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35e+04     |
|    n_updates            | 11840        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.85         |
|    value_loss           | 2.72e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -946         |
|    std_reward           | 72.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023282873 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 11850        |
|    policy_gradient_loss | -0.000644    |
|    std                  | 0.849        |
|    value_loss           | 2.75e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1e+03       |
|    std_reward           | 103          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0012866542 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47e+04     |
|    n_updates            | 11860        |
|    policy_gradient_loss | -0.000767    |
|    std                  | 0.848        |
|    value_loss           | 3.05e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 69.6         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0028451784 |
|    clip_fraction        | 0.000952     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 11870        |
|    policy_gradient_loss | -0.00105     |
|    std                  | 0.847        |
|    value_loss           | 2.87e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -994         |
|    std_reward           | 79.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0020729494 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+04      |
|    n_updates            | 11880        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.848        |
|    value_loss           | 2.86e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1e+03      |
|    std_reward           | 136         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008127077 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35e+04    |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.003      |
|    std                  | 0.849       |
|    value_loss           | 2.76e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -960        |
|    std_reward           | 131         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004829406 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36e+04    |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.00144    |
|    std                  | 0.848       |
|    value_loss           | 2.8e+04     |
-----------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -984       |
|    std_reward           | 107        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00754902 |
|    clip_fraction        | 0.0424     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.5       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.3e+04    |
|    n_updates            | 11910      |
|    policy_gradient_loss | -0.00265   |
|    std                  | 0.849      |
|    value_loss           | 2.74e+04   |
----------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_reward          | -961      |
|    std_reward           | 140       |
|    success_rate         | 0         |
| train/                  |           |
|    approx_kl            | 0.0047824 |
|    clip_fraction        | 0.00906   |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.5      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0003    |
|    loss                 | 1.35e+04  |
|    n_updates            | 11920     |
|    policy_gradient_loss | -0.0011   |
|    std                  | 0.849     |
|    value_loss           | 2.65e+04  |
---------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -900         |
|    std_reward           | 61.5         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038029582 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.37e+04     |
|    n_updates            | 11930        |
|    policy_gradient_loss | -0.000862    |
|    std                  | 0.848        |
|    value_loss           | 2.76e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -902         |
|    std_reward           | 88.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0014299897 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+04      |
|    n_updates            | 11940        |
|    policy_gradient_loss | -0.000761    |
|    std                  | 0.848        |
|    value_loss           | 2.85e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -910        |
|    std_reward           | 45.7        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006615691 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29e+04    |
|    n_updates            | 11950       |
|    policy_gradient_loss | -0.00154    |
|    std                  | 0.847       |
|    value_loss           | 2.62e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -900         |
|    std_reward           | 124          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0015132696 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28e+04     |
|    n_updates            | 11960        |
|    policy_gradient_loss | -0.000584    |
|    std                  | 0.847        |
|    value_loss           | 2.57e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -973        |
|    std_reward           | 126         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004074864 |
|    clip_fraction        | 0.00532     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33e+04    |
|    n_updates            | 11970       |
|    policy_gradient_loss | -0.000996   |
|    std                  | 0.848       |
|    value_loss           | 2.72e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 179          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0053442977 |
|    clip_fraction        | 0.00989      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.39e+04     |
|    n_updates            | 11980        |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.848        |
|    value_loss           | 2.75e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.08e+03   |
|    std_reward           | 180         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005899871 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43e+04    |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.00137    |
|    std                  | 0.849       |
|    value_loss           | 2.79e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.28e+03   |
|    std_reward           | 198         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004524785 |
|    clip_fraction        | 0.00691     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.41e+04    |
|    n_updates            | 12000       |
|    policy_gradient_loss | -0.00138    |
|    std                  | 0.849       |
|    value_loss           | 2.77e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.06e+03   |
|    std_reward           | 222         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002374177 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.47e+04    |
|    n_updates            | 12010       |
|    policy_gradient_loss | -0.000863   |
|    std                  | 0.848       |
|    value_loss           | 2.87e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.39e+03    |
|    std_reward           | 183          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033425754 |
|    clip_fraction        | 0.00188      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 12020        |
|    policy_gradient_loss | -0.000771    |
|    std                  | 0.847        |
|    value_loss           | 2.91e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.16e+03    |
|    std_reward           | 178          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034401934 |
|    clip_fraction        | 0.00217      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 12030        |
|    policy_gradient_loss | -0.00103     |
|    std                  | 0.846        |
|    value_loss           | 2.89e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.21e+03    |
|    std_reward           | 276          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0054130442 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+04      |
|    n_updates            | 12040        |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.846        |
|    value_loss           | 2.89e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.16e+03    |
|    std_reward           | 250          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0041083805 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+04     |
|    n_updates            | 12050        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.846        |
|    value_loss           | 2.9e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -878         |
|    std_reward           | 62.3         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0056159035 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.37e+04     |
|    n_updates            | 12060        |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.845        |
|    value_loss           | 2.74e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.38e+03   |
|    std_reward           | 296         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004231845 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.47       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38e+04    |
|    n_updates            | 12070       |
|    policy_gradient_loss | -0.000993   |
|    std                  | 0.847       |
|    value_loss           | 2.82e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.13e+03    |
|    std_reward           | 265          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023570666 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.48        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.43e+04     |
|    n_updates            | 12080        |
|    policy_gradient_loss | -0.000776    |
|    std                  | 0.848        |
|    value_loss           | 2.87e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.21e+03    |
|    std_reward           | 353          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0034063894 |
|    clip_fraction        | 0.00212      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55e+04     |
|    n_updates            | 12090        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.848        |
|    value_loss           | 2.99e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.06e+03    |
|    std_reward           | 268          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038238536 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 12100        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.849        |
|    value_loss           | 2.91e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -930        |
|    std_reward           | 148         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005705552 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.48e+04    |
|    n_updates            | 12110       |
|    policy_gradient_loss | -0.00149    |
|    std                  | 0.85        |
|    value_loss           | 2.97e+04    |
-----------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -869       |
|    std_reward           | 57.9       |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00718241 |
|    clip_fraction        | 0.0257     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.5       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42e+04   |
|    n_updates            | 12120      |
|    policy_gradient_loss | -0.0025    |
|    std                  | 0.85       |
|    value_loss           | 2.78e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -667         |
|    std_reward           | 624          |
|    success_rate         | 0.1          |
| train/                  |              |
|    approx_kl            | 0.0039460217 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.67e+04     |
|    n_updates            | 12130        |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.852        |
|    value_loss           | 3.27e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -960        |
|    std_reward           | 205         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006427821 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 12140       |
|    policy_gradient_loss | -0.00208    |
|    std                  | 0.853       |
|    value_loss           | 2.79e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -975        |
|    std_reward           | 88.2        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006354683 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.32e+04    |
|    n_updates            | 12150       |
|    policy_gradient_loss | -0.00304    |
|    std                  | 0.853       |
|    value_loss           | 2.67e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 180          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0039248765 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35e+04     |
|    n_updates            | 12160        |
|    policy_gradient_loss | -0.00159     |
|    std                  | 0.854        |
|    value_loss           | 2.71e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -977         |
|    std_reward           | 156          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0026243273 |
|    clip_fraction        | 0.00168      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.45e+04     |
|    n_updates            | 12170        |
|    policy_gradient_loss | -0.000434    |
|    std                  | 0.854        |
|    value_loss           | 2.83e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -943         |
|    std_reward           | 124          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0026704536 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47e+04     |
|    n_updates            | 12180        |
|    policy_gradient_loss | -0.000851    |
|    std                  | 0.854        |
|    value_loss           | 2.86e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.11e+03   |
|    std_reward           | 190         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005305753 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39e+04    |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.00141    |
|    std                  | 0.855       |
|    value_loss           | 2.83e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.14e+03   |
|    std_reward           | 255         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006006578 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.54       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.0014     |
|    std                  | 0.854       |
|    value_loss           | 2.85e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.26e+03    |
|    std_reward           | 376          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0064062187 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 12210        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.853        |
|    value_loss           | 2.8e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -946        |
|    std_reward           | 117         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005724919 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.51e+04    |
|    n_updates            | 12220       |
|    policy_gradient_loss | -0.0012     |
|    std                  | 0.853       |
|    value_loss           | 2.95e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -975         |
|    std_reward           | 157          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0055464953 |
|    clip_fraction        | 0.00964      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.52        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.34e+04     |
|    n_updates            | 12230        |
|    policy_gradient_loss | -0.00141     |
|    std                  | 0.852        |
|    value_loss           | 2.67e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -922         |
|    std_reward           | 64           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0053441054 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.36e+04     |
|    n_updates            | 12240        |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.851        |
|    value_loss           | 2.76e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -994        |
|    std_reward           | 147         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008095602 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36e+04    |
|    n_updates            | 12250       |
|    policy_gradient_loss | -0.00306    |
|    std                  | 0.85        |
|    value_loss           | 2.83e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.14e+03    |
|    std_reward           | 243          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0052783354 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12260        |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.846        |
|    value_loss           | 2.76e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.02e+03    |
|    std_reward           | 125          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018076997 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.43e+04     |
|    n_updates            | 12270        |
|    policy_gradient_loss | -0.000905    |
|    std                  | 0.844        |
|    value_loss           | 2.87e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.19e+03    |
|    std_reward           | 341          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0072651203 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 12280        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.843        |
|    value_loss           | 2.96e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.43e+03    |
|    std_reward           | 295          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0052978927 |
|    clip_fraction        | 0.00872      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+04     |
|    n_updates            | 12290        |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.843        |
|    value_loss           | 3.17e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.31e+03    |
|    std_reward           | 341          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0061840313 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72e+04     |
|    n_updates            | 12300        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 0.844        |
|    value_loss           | 3.33e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.58e+03    |
|    std_reward           | 271          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0045661465 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.46        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+04     |
|    n_updates            | 12310        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.844        |
|    value_loss           | 2.84e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.41e+03   |
|    std_reward           | 288         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005090818 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45e+04    |
|    n_updates            | 12320       |
|    policy_gradient_loss | -0.00127    |
|    std                  | 0.843       |
|    value_loss           | 2.92e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.67e+03    |
|    std_reward           | 354          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0052157626 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+04     |
|    n_updates            | 12330        |
|    policy_gradient_loss | -0.0014      |
|    std                  | 0.844        |
|    value_loss           | 2.84e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.21e+03   |
|    std_reward           | 264         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.009449326 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.46       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7e+04     |
|    n_updates            | 12340       |
|    policy_gradient_loss | -0.00332    |
|    std                  | 0.844       |
|    value_loss           | 3.36e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.07e+03   |
|    std_reward           | 257         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005259473 |
|    clip_fraction        | 0.00972     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.46       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37e+04    |
|    n_updates            | 12350       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 0.845       |
|    value_loss           | 2.77e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.31e+03    |
|    std_reward           | 321          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0069245654 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.46        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12360        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.844        |
|    value_loss           | 2.68e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.49e+03  |
|    std_reward           | 235        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00594471 |
|    clip_fraction        | 0.0108     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.45      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35e+04   |
|    n_updates            | 12370      |
|    policy_gradient_loss | -0.00132   |
|    std                  | 0.844      |
|    value_loss           | 2.82e+04   |
----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.28e+03   |
|    std_reward           | 229         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003608489 |
|    clip_fraction        | 0.00222     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.42e+04    |
|    n_updates            | 12380       |
|    policy_gradient_loss | -0.000965   |
|    std                  | 0.844       |
|    value_loss           | 2.87e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.1e+03     |
|    std_reward           | 254          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0049295453 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+04     |
|    n_updates            | 12390        |
|    policy_gradient_loss | -0.00211     |
|    std                  | 0.843        |
|    value_loss           | 2.76e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.16e+03    |
|    std_reward           | 204          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0048698997 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 12400        |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.843        |
|    value_loss           | 2.76e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.11e+03  |
|    std_reward           | 168        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00369109 |
|    clip_fraction        | 0.00447    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.45      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.4e+04    |
|    n_updates            | 12410      |
|    policy_gradient_loss | -0.000446  |
|    std                  | 0.843      |
|    value_loss           | 2.82e+04   |
----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.15e+03   |
|    std_reward           | 248         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005041907 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37e+04    |
|    n_updates            | 12420       |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.843       |
|    value_loss           | 2.73e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.09e+03    |
|    std_reward           | 80.7         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0048536984 |
|    clip_fraction        | 0.00925      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.44        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.32e+04     |
|    n_updates            | 12430        |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.842        |
|    value_loss           | 2.63e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -985        |
|    std_reward           | 128         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005686497 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.43       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45e+04    |
|    n_updates            | 12440       |
|    policy_gradient_loss | -0.00154    |
|    std                  | 0.84        |
|    value_loss           | 2.92e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.06e+03    |
|    std_reward           | 212          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0058625597 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.34e+04     |
|    n_updates            | 12450        |
|    policy_gradient_loss | -0.0025      |
|    std                  | 0.839        |
|    value_loss           | 2.63e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -940         |
|    std_reward           | 72           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0039162547 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.41        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26e+04     |
|    n_updates            | 12460        |
|    policy_gradient_loss | -0.000765    |
|    std                  | 0.838        |
|    value_loss           | 2.56e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -911         |
|    std_reward           | 70.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0068977783 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.37e+04     |
|    n_updates            | 12470        |
|    policy_gradient_loss | -0.00232     |
|    std                  | 0.836        |
|    value_loss           | 2.68e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.26e+03   |
|    std_reward           | 283         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008588677 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.39       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27e+04    |
|    n_updates            | 12480       |
|    policy_gradient_loss | -0.00218    |
|    std                  | 0.836       |
|    value_loss           | 2.7e+04     |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.13e+03    |
|    std_reward           | 261          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0061147264 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+04     |
|    n_updates            | 12490        |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.836        |
|    value_loss           | 2.87e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -998       |
|    std_reward           | 141        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00474024 |
|    clip_fraction        | 0.00669    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.39      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.31e+04   |
|    n_updates            | 12500      |
|    policy_gradient_loss | -0.00118   |
|    std                  | 0.836      |
|    value_loss           | 2.63e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -947         |
|    std_reward           | 106          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0020015687 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12510        |
|    policy_gradient_loss | -0.000851    |
|    std                  | 0.837        |
|    value_loss           | 2.68e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -950         |
|    std_reward           | 108          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0077569215 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.31e+04     |
|    n_updates            | 12520        |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.837        |
|    value_loss           | 2.65e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.06e+03   |
|    std_reward           | 197         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005809832 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.4        |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.45e+04    |
|    n_updates            | 12530       |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.836       |
|    value_loss           | 2.86e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.12e+03    |
|    std_reward           | 229          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0045151142 |
|    clip_fraction        | 0.00823      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.36e+04     |
|    n_updates            | 12540        |
|    policy_gradient_loss | -0.00141     |
|    std                  | 0.836        |
|    value_loss           | 2.7e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.17e+03   |
|    std_reward           | 209         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006580687 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.39       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.31e+04    |
|    n_updates            | 12550       |
|    policy_gradient_loss | -0.00177    |
|    std                  | 0.836       |
|    value_loss           | 2.78e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.18e+03    |
|    std_reward           | 145          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0059602754 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.35e+04     |
|    n_updates            | 12560        |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.835        |
|    value_loss           | 2.81e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -989        |
|    std_reward           | 131         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002971619 |
|    clip_fraction        | 0.00144     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.38       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38e+04    |
|    n_updates            | 12570       |
|    policy_gradient_loss | -0.000889   |
|    std                  | 0.834       |
|    value_loss           | 2.68e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -949         |
|    std_reward           | 109          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0040822644 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.37        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12580        |
|    policy_gradient_loss | -0.000992    |
|    std                  | 0.832        |
|    value_loss           | 2.7e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -928         |
|    std_reward           | 98           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0052374313 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.27e+04     |
|    n_updates            | 12590        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.83         |
|    value_loss           | 2.53e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.02e+03    |
|    std_reward           | 130          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0048970403 |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.33e+04     |
|    n_updates            | 12600        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 0.83         |
|    value_loss           | 2.64e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.03e+03    |
|    std_reward           | 137          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0038604196 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28e+04     |
|    n_updates            | 12610        |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.83         |
|    value_loss           | 2.5e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.19e+03   |
|    std_reward           | 209         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005106031 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29e+04    |
|    n_updates            | 12620       |
|    policy_gradient_loss | -0.00118    |
|    std                  | 0.829       |
|    value_loss           | 2.57e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.23e+03    |
|    std_reward           | 247          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0018260486 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.35        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.3e+04      |
|    n_updates            | 12630        |
|    policy_gradient_loss | -0.000563    |
|    std                  | 0.829        |
|    value_loss           | 2.63e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.32e+03    |
|    std_reward           | 209          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0044725835 |
|    clip_fraction        | 0.00574      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.32e+04     |
|    n_updates            | 12640        |
|    policy_gradient_loss | -0.000782    |
|    std                  | 0.829        |
|    value_loss           | 2.63e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.17e+03   |
|    std_reward           | 189         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002571262 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.36e+04    |
|    n_updates            | 12650       |
|    policy_gradient_loss | -0.000712   |
|    std                  | 0.829       |
|    value_loss           | 2.76e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.11e+03    |
|    std_reward           | 183          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0043232907 |
|    clip_fraction        | 0.00642      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12660        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.828        |
|    value_loss           | 2.65e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.22e+03   |
|    std_reward           | 237         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003424882 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19e+04    |
|    n_updates            | 12670       |
|    policy_gradient_loss | -0.0021     |
|    std                  | 0.828       |
|    value_loss           | 2.35e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.05e+03    |
|    std_reward           | 182          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0053159404 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.16e+04     |
|    n_updates            | 12680        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 0.829        |
|    value_loss           | 2.42e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.1e+03    |
|    std_reward           | 165         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004288771 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25e+04    |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.00148    |
|    std                  | 0.828       |
|    value_loss           | 2.37e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.03e+03    |
|    std_reward           | 168          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0036561915 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.31e+04     |
|    n_updates            | 12700        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.828        |
|    value_loss           | 2.42e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.14e+03   |
|    std_reward           | 171         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003706351 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+04     |
|    n_updates            | 12710       |
|    policy_gradient_loss | -0.000889   |
|    std                  | 0.83        |
|    value_loss           | 2.5e+04     |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.24e+03    |
|    std_reward           | 250          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0063603735 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.29e+04     |
|    n_updates            | 12720        |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.828        |
|    value_loss           | 2.49e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.17e+03    |
|    std_reward           | 190          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0057794317 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.33e+04     |
|    n_updates            | 12730        |
|    policy_gradient_loss | -0.00205     |
|    std                  | 0.827        |
|    value_loss           | 2.55e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.23e+03    |
|    std_reward           | 197          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0056406935 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.2e+04      |
|    n_updates            | 12740        |
|    policy_gradient_loss | -0.00323     |
|    std                  | 0.827        |
|    value_loss           | 2.37e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -998         |
|    std_reward           | 180          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0050131152 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+04     |
|    n_updates            | 12750        |
|    policy_gradient_loss | -0.000798    |
|    std                  | 0.825        |
|    value_loss           | 2.66e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -904         |
|    std_reward           | 54.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0057958076 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.27e+04     |
|    n_updates            | 12760        |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.825        |
|    value_loss           | 2.49e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.12e+03  |
|    std_reward           | 203        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00596165 |
|    clip_fraction        | 0.0569     |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.31      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28e+04   |
|    n_updates            | 12770      |
|    policy_gradient_loss | -0.00336   |
|    std                  | 0.825      |
|    value_loss           | 2.49e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.02e+03    |
|    std_reward           | 164          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0025041215 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.25e+04     |
|    n_updates            | 12780        |
|    policy_gradient_loss | -0.000393    |
|    std                  | 0.824        |
|    value_loss           | 2.5e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.01e+03   |
|    std_reward           | 155         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003665321 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18e+04    |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.00364    |
|    std                  | 0.822       |
|    value_loss           | 2.44e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -935         |
|    std_reward           | 89.8         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0057198666 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18e+04     |
|    n_updates            | 12800        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.821        |
|    value_loss           | 2.4e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.02e+03    |
|    std_reward           | 112          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0037338873 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.16e+04     |
|    n_updates            | 12810        |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.823        |
|    value_loss           | 2.3e+04      |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -995        |
|    std_reward           | 79.8        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004587884 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+04     |
|    n_updates            | 12820       |
|    policy_gradient_loss | -0.00262    |
|    std                  | 0.822       |
|    value_loss           | 2.41e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -962         |
|    std_reward           | 143          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0055857515 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.27        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+04     |
|    n_updates            | 12830        |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.82         |
|    value_loss           | 2.49e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -997        |
|    std_reward           | 95.7        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004927147 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23e+04    |
|    n_updates            | 12840       |
|    policy_gradient_loss | -0.00176    |
|    std                  | 0.818       |
|    value_loss           | 2.45e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.19e+03   |
|    std_reward           | 195         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004976064 |
|    clip_fraction        | 0.00901     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17e+04    |
|    n_updates            | 12850       |
|    policy_gradient_loss | -0.000898   |
|    std                  | 0.818       |
|    value_loss           | 2.42e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -992         |
|    std_reward           | 118          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0037506316 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18e+04     |
|    n_updates            | 12860        |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.818        |
|    value_loss           | 2.44e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.01e+03   |
|    std_reward           | 114         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004660278 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15e+04    |
|    n_updates            | 12870       |
|    policy_gradient_loss | -0.0031     |
|    std                  | 0.818       |
|    value_loss           | 2.36e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.08e+03   |
|    std_reward           | 149         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002904857 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+04    |
|    n_updates            | 12880       |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.818       |
|    value_loss           | 2.37e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.11e+03   |
|    std_reward           | 144         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004394193 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+04    |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.00249    |
|    std                  | 0.817       |
|    value_loss           | 2.32e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.11e+03    |
|    std_reward           | 129          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0053069014 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.24        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 12900        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.817        |
|    value_loss           | 2.31e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.09e+03   |
|    std_reward           | 58.7        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004766602 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14e+04    |
|    n_updates            | 12910       |
|    policy_gradient_loss | -0.00116    |
|    std                  | 0.816       |
|    value_loss           | 2.34e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.17e+03   |
|    std_reward           | 105         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003430399 |
|    clip_fraction        | 0.0459      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09e+04    |
|    n_updates            | 12920       |
|    policy_gradient_loss | -0.00278    |
|    std                  | 0.816       |
|    value_loss           | 2.25e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.16e+03    |
|    std_reward           | 176          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0024201854 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.2e+04      |
|    n_updates            | 12930        |
|    policy_gradient_loss | -0.000484    |
|    std                  | 0.815        |
|    value_loss           | 2.34e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.27e+03   |
|    std_reward           | 165         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006376276 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15e+04    |
|    n_updates            | 12940       |
|    policy_gradient_loss | -0.00186    |
|    std                  | 0.812       |
|    value_loss           | 2.27e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.25e+03   |
|    std_reward           | 144         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004820181 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28e+04    |
|    n_updates            | 12950       |
|    policy_gradient_loss | -0.00121    |
|    std                  | 0.811       |
|    value_loss           | 2.5e+04     |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.19e+03   |
|    std_reward           | 116         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005013638 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14e+04    |
|    n_updates            | 12960       |
|    policy_gradient_loss | -0.0014     |
|    std                  | 0.81        |
|    value_loss           | 2.32e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.07e+03   |
|    std_reward           | 89.3        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006186589 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17e+04    |
|    n_updates            | 12970       |
|    policy_gradient_loss | -0.00313    |
|    std                  | 0.81        |
|    value_loss           | 2.35e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.12e+03   |
|    std_reward           | 137         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.008688479 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.16e+04    |
|    n_updates            | 12980       |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.812       |
|    value_loss           | 2.26e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.12e+03   |
|    std_reward           | 131         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.004575219 |
|    clip_fraction        | 0.00593     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24e+04    |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.000752   |
|    std                  | 0.811       |
|    value_loss           | 2.29e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.18e+03   |
|    std_reward           | 90.6        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.003585922 |
|    clip_fraction        | 0.00259     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18e+04    |
|    n_updates            | 13000       |
|    policy_gradient_loss | -0.00103    |
|    std                  | 0.809       |
|    value_loss           | 2.35e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.16e+03    |
|    std_reward           | 51.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0066050002 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.25e+04     |
|    n_updates            | 13010        |
|    policy_gradient_loss | -0.00254     |
|    std                  | 0.809        |
|    value_loss           | 2.39e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -989         |
|    std_reward           | 74           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0044620684 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 13020        |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.808        |
|    value_loss           | 2.31e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -929         |
|    std_reward           | 61.4         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0033084326 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.08e+04     |
|    n_updates            | 13030        |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.806        |
|    value_loss           | 2.28e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -938         |
|    std_reward           | 38           |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0040498036 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.09e+04     |
|    n_updates            | 13040        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 0.805        |
|    value_loss           | 2.22e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -994        |
|    std_reward           | 43.5        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.002229108 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09e+04    |
|    n_updates            | 13050       |
|    policy_gradient_loss | -0.00076    |
|    std                  | 0.805       |
|    value_loss           | 2.17e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -807        |
|    std_reward           | 659         |
|    success_rate         | 0.1         |
| train/                  |             |
|    approx_kl            | 0.005827845 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19e+04    |
|    n_updates            | 13060       |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.805       |
|    value_loss           | 2.32e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.14e+03   |
|    std_reward           | 114         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.006647992 |
|    clip_fraction        | 0.0528      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17e+04    |
|    n_updates            | 13070       |
|    policy_gradient_loss | -0.00296    |
|    std                  | 0.806       |
|    value_loss           | 2.23e+04    |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.11e+03   |
|    std_reward           | 101         |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005403694 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17e+04    |
|    n_updates            | 13080       |
|    policy_gradient_loss | -0.00173    |
|    std                  | 0.806       |
|    value_loss           | 2.38e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.25e+03    |
|    std_reward           | 57.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0043103835 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+04      |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.805        |
|    value_loss           | 2.97e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -962         |
|    std_reward           | 130          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0032064691 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+04     |
|    n_updates            | 13100        |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.805        |
|    value_loss           | 2.43e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -924        |
|    std_reward           | 702         |
|    success_rate         | 0.1         |
| train/                  |             |
|    approx_kl            | 0.011547584 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.52e+04    |
|    n_updates            | 13110       |
|    policy_gradient_loss | -9.04e-06   |
|    std                  | 0.794       |
|    value_loss           | 2.83e+04    |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.14e+03    |
|    std_reward           | 154          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0066654077 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.16e+04     |
|    n_updates            | 13120        |
|    policy_gradient_loss | 0.00445      |
|    std                  | 0.791        |
|    value_loss           | 2.38e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.13e+03    |
|    std_reward           | 94.8         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0061293603 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 13130        |
|    policy_gradient_loss | -0.00165     |
|    std                  | 0.791        |
|    value_loss           | 2.28e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.2e+03     |
|    std_reward           | 76.8         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0009879904 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 13140        |
|    policy_gradient_loss | -0.000112    |
|    std                  | 0.79         |
|    value_loss           | 2.29e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.14e+03    |
|    std_reward           | 164          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0058735516 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.14e+04     |
|    n_updates            | 13150        |
|    policy_gradient_loss | -0.00215     |
|    std                  | 0.791        |
|    value_loss           | 2.31e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.27e+03    |
|    std_reward           | 67.3         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0036034365 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1e+04      |
|    n_updates            | 13160        |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.791        |
|    value_loss           | 2.32e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.39e+03  |
|    std_reward           | 51.6       |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00409321 |
|    clip_fraction        | 0.00474    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.04      |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17e+04   |
|    n_updates            | 13170      |
|    policy_gradient_loss | -0.00132   |
|    std                  | 0.791      |
|    value_loss           | 2.48e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.27e+03    |
|    std_reward           | 73.1         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0016009202 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.17e+04     |
|    n_updates            | 13180        |
|    policy_gradient_loss | -0.000858    |
|    std                  | 0.791        |
|    value_loss           | 2.38e+04     |
------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.18e+03  |
|    std_reward           | 105        |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00238495 |
|    clip_fraction        | 0.000659   |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.04      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19e+04   |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.000556  |
|    std                  | 0.791      |
|    value_loss           | 2.46e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.2e+03     |
|    std_reward           | 108          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0013351014 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 1.2e+04      |
|    n_updates            | 13200        |
|    policy_gradient_loss | -0.000728    |
|    std                  | 0.791        |
|    value_loss           | 2.29e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.29e+03    |
|    std_reward           | 71.9         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0013823584 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+04     |
|    n_updates            | 13210        |
|    policy_gradient_loss | -0.000744    |
|    std                  | 0.791        |
|    value_loss           | 2.34e+04     |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_reward          | -1.26e+03   |
|    std_reward           | 43.3        |
|    success_rate         | 0           |
| train/                  |             |
|    approx_kl            | 0.005210565 |
|    clip_fraction        | 0.00857     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14e+04    |
|    n_updates            | 13220       |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.791       |
|    value_loss           | 2.46e+04    |
-----------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_reward          | -1.21e+03  |
|    std_reward           | 35.9       |
|    success_rate         | 0          |
| train/                  |            |
|    approx_kl            | 0.00479958 |
|    clip_fraction        | 0.00879    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.04      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+04   |
|    n_updates            | 13230      |
|    policy_gradient_loss | -0.00143   |
|    std                  | 0.791      |
|    value_loss           | 2.35e+04   |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.17e+03    |
|    std_reward           | 112          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0014409545 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.22e+04     |
|    n_updates            | 13240        |
|    policy_gradient_loss | -0.00072     |
|    std                  | 0.791        |
|    value_loss           | 2.37e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.32e+03    |
|    std_reward           | 111          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0031808382 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.24e+04     |
|    n_updates            | 13250        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.791        |
|    value_loss           | 2.41e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.34e+03    |
|    std_reward           | 170          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0023300424 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.25e+04     |
|    n_updates            | 13260        |
|    policy_gradient_loss | -0.000848    |
|    std                  | 0.79         |
|    value_loss           | 2.52e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.25e+03    |
|    std_reward           | 134          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0011227577 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26e+04     |
|    n_updates            | 13270        |
|    policy_gradient_loss | -0.000674    |
|    std                  | 0.79         |
|    value_loss           | 2.5e+04      |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.15e+03    |
|    std_reward           | 117          |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0069356775 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+04     |
|    n_updates            | 13280        |
|    policy_gradient_loss | -0.00243     |
|    std                  | 0.79         |
|    value_loss           | 2.43e+04     |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    mean_reward          | -1.21e+03    |
|    std_reward           | 93.2         |
|    success_rate         | 0            |
| train/                  |              |
|    approx_kl            | 0.0053172014 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0003       |
|    loss                 | 1.22e+04     |
|    n_updates            | 13290        |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.791        |
|    value_loss           | 2.4e+04      |
------------------------------------------
